{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7WtHrk0AVXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf01dd47-6014-473a-bc65-b876a8e8954e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n",
            "Confusion Matrix:\n",
            "[[162   0]\n",
            " [ 38   0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class HiddenNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.classes = None\n",
        "        self.class_priors = None\n",
        "        self.feature_likelihoods = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.class_priors = self.calculate_class_priors(y)\n",
        "        self.feature_likelihoods = self.calculate_feature_likelihoods(X, y)\n",
        "\n",
        "    def calculate_class_priors(self, y):\n",
        "        class_counts = {}\n",
        "        for label in y:\n",
        "            if label in class_counts:\n",
        "                class_counts[label] += 1\n",
        "            else:\n",
        "                class_counts[label] = 1\n",
        "\n",
        "        total_samples = len(y)\n",
        "        class_priors = {}\n",
        "        for label, count in class_counts.items():\n",
        "            class_priors[label] = count / total_samples\n",
        "\n",
        "        return class_priors\n",
        "\n",
        "    def calculate_feature_likelihoods(self, X, y):\n",
        "        feature_likelihoods = {}\n",
        "        for label in self.classes:\n",
        "            feature_likelihoods[label] = {}\n",
        "            relevant_samples = X[y == label]\n",
        "\n",
        "            for feature in range(X.shape[1]):\n",
        "                feature_values = relevant_samples[:, feature]\n",
        "                feature_counts = {}\n",
        "                for value in feature_values:\n",
        "                    if value in feature_counts:\n",
        "                        feature_counts[value] += 1\n",
        "                    else:\n",
        "                        feature_counts[value] = 1\n",
        "\n",
        "                total_samples = len(feature_values)\n",
        "                for value, count in feature_counts.items():\n",
        "                    feature_likelihoods[label][feature, value] = count / total_samples\n",
        "\n",
        "        return feature_likelihoods\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            scores = {}\n",
        "            for label in self.classes:\n",
        "                scores[label] = np.log(self.class_priors[label])\n",
        "\n",
        "                for feature, value in enumerate(sample):\n",
        "                    if (feature, value) in self.feature_likelihoods[label]:\n",
        "                        scores[label] += np.log(self.feature_likelihoods[label][feature, value])\n",
        "\n",
        "            predicted_label = max(scores, key=scores.get)\n",
        "            predictions.append(predicted_label)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Insurance_claims.csv')\n",
        "d=df\n",
        "\n",
        "# Preprocess the dataset\n",
        "le = LabelEncoder()\n",
        "df['fraud_reported'] = le.fit_transform(df['fraud_reported'])\n",
        "df['policy_state'] = le.fit_transform(d['policy_state'])\n",
        "df['policy_csl'] = le.fit_transform(d['policy_csl'])\n",
        "df['insured_sex'] = le.fit_transform(d['insured_sex'])\n",
        "df['insured_education_level'] = le.fit_transform(d['insured_education_level'])\n",
        "df['insured_occupation'] = le.fit_transform(d['insured_occupation'])\n",
        "df['insured_hobbies'] = le.fit_transform(d['insured_hobbies'])\n",
        "df['insured_relationship'] = le.fit_transform(d['insured_relationship'])\n",
        "df['incident_date'] = le.fit_transform(d['incident_date'])\n",
        "df['incident_type'] = le.fit_transform(d['incident_type'])\n",
        "df['collision_type'] = le.fit_transform(d['collision_type'])\n",
        "df['incident_severity'] = le.fit_transform(d['incident_severity'])\n",
        "df['authorities_contacted'] = le.fit_transform(d['authorities_contacted'])\n",
        "df['incident_state'] = le.fit_transform(d['incident_state'])\n",
        "df['incident_city'] = le.fit_transform(d['incident_city'])\n",
        "df['incident_location'] = le.fit_transform(d['incident_location'])\n",
        "df['property_damage'] = le.fit_transform(d['property_damage'])\n",
        "df['police_report_available'] = le.fit_transform(d['police_report_available'])\n",
        "df['auto_make'] = le.fit_transform(d['auto_make'])\n",
        "df['auto_model'] = le.fit_transform(d['auto_model'])\n",
        "\n",
        "# Select the relevant features for classification\n",
        "selected_features = ['months_as_customer', 'age','policy_number','policy_bind_date','policy_state','policy_csl',\n",
        "                     'policy_deductable','policy_annual_premium', 'umbrella_limit','insured_zip',  'insured_sex',\n",
        "                     'insured_education_level', 'insured_occupation','insured_occupation', 'insured_relationship',\n",
        "                     'capital-gains', 'capital-loss','incident_date', 'incident_type', 'collision_type',\n",
        "                     'incident_severity', 'authorities_contacted','incident_state','incident_city','incident_location',\n",
        "                     'incident_hour_of_the_day', 'number_of_vehicles_involved', 'property_damage',\n",
        "                     'bodily_injuries', 'witnesses', 'police_report_available', 'total_claim_amount',\n",
        "                     'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make', 'auto_model', 'auto_year',\n",
        "                     'fraud_reported']\n",
        "\n",
        "df = df[selected_features]\n",
        "\n",
        "# Convert incident_date to numerical feature\n",
        "df['incident_date'] = pd.to_datetime(df['incident_date'])\n",
        "df['incident_date'] = (df['incident_date'] - df['incident_date'].min()).dt.days\n",
        "\n",
        "# Drop the features you want to remove\n",
        "features_to_drop = ['insured_zip','incident_state','incident_city','incident_location','policy_bind_date','incident_date','insured_occupation','policy_number','insured_education_level']\n",
        "df = df.drop(features_to_drop, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into features and labels\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
        "\n",
        "sc=StandardScaler()\n",
        "\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)\n",
        "\n",
        "# Train the Hidden Naive Bayes classifier\n",
        "hnb = HiddenNaiveBayes()\n",
        "hnb.train(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = hnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSdEDMBJPW43"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}